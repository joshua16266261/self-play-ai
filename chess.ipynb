{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import chess.svg\n",
    "import chess.engine\n",
    "from IPython.display import SVG, display, clear_output\n",
    "from typing import Tuple, Dict, Hashable, List, Any\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import tensorflow as tf\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hash(board: chess.Board) -> Hashable:\n",
    "    return (board.pawns, board.knights, board.bishops, board.rooks,\n",
    "            board.queens, board.kings,\n",
    "            board.occupied_co[chess.WHITE], board.occupied_co[chess.BLACK],\n",
    "            board.turn, board.clean_castling_rights(),\n",
    "            board.ep_square if board.has_legal_en_passant() else None)\n",
    "\n",
    "NUM_PROMOTION_DIRS = 3\n",
    "NUM_SINGLE_SQUARE_STEPS = 7\n",
    "\n",
    "ROOK_PROMOTION_START_IDX = 0\n",
    "BISHOP_PROMOTION_START_IDX = ROOK_PROMOTION_START_IDX + NUM_PROMOTION_DIRS\n",
    "KNIGHT_PROMOTION_START_IDX = BISHOP_PROMOTION_START_IDX + NUM_PROMOTION_DIRS\n",
    "\n",
    "HORIZONTAL_MOVE_START_IDX = KNIGHT_PROMOTION_START_IDX + NUM_PROMOTION_DIRS\n",
    "VERTICAL_MOVE_START_IDX = HORIZONTAL_MOVE_START_IDX + 2 * NUM_SINGLE_SQUARE_STEPS\n",
    "DIAGONAL_MOVE_START_IDX = VERTICAL_MOVE_START_IDX + 2 * NUM_SINGLE_SQUARE_STEPS\n",
    "KNIGHT_MOVE_START_IDX = DIAGONAL_MOVE_START_IDX + 4 * NUM_SINGLE_SQUARE_STEPS\n",
    "\n",
    "def move_to_policy_idx(move: chess.Move, is_black: bool) -> int:\n",
    "    \"\"\"Convert a move to an int in [0, 73) representing the index in the policy.\"\"\"\n",
    "    rank_diff = chess.square_rank(move.to_square) - chess.square_rank(move.from_square)\n",
    "    file_diff = chess.square_file(move.to_square) - chess.square_file(move.from_square)\n",
    "    abs_rank_diff, abs_file_diff = abs(rank_diff), abs(file_diff)\n",
    "\n",
    "    if is_black:\n",
    "        rank_diff *= -1\n",
    "        # file_diff *= -1\n",
    "\n",
    "    # Under-promotion\n",
    "    if move.promotion is not None and move.promotion is not chess.QUEEN:\n",
    "        sub_idx = file_diff % 3 # Straight: 0, Right: 1, Left: 2\n",
    "        if move.promotion is chess.ROOK:\n",
    "            return ROOK_PROMOTION_START_IDX + sub_idx\n",
    "        if move.promotion is chess.BISHOP:\n",
    "            return BISHOP_PROMOTION_START_IDX + sub_idx\n",
    "        return KNIGHT_PROMOTION_START_IDX + sub_idx\n",
    "    \n",
    "    # Horizontal\n",
    "    if rank_diff == 0:\n",
    "        if file_diff < 0:\n",
    "            return HORIZONTAL_MOVE_START_IDX + -file_diff - 1\n",
    "        return HORIZONTAL_MOVE_START_IDX + NUM_SINGLE_SQUARE_STEPS + file_diff - 1\n",
    "    \n",
    "    # Vertical\n",
    "    if file_diff == 0:\n",
    "        if rank_diff < 0:\n",
    "            return VERTICAL_MOVE_START_IDX + -rank_diff - 1\n",
    "        return VERTICAL_MOVE_START_IDX + NUM_SINGLE_SQUARE_STEPS + rank_diff - 1\n",
    "    \n",
    "    # Diagonal\n",
    "    if abs_rank_diff == abs_file_diff:\n",
    "        if file_diff < 0:\n",
    "            if rank_diff > 0: # Northwest\n",
    "                return DIAGONAL_MOVE_START_IDX + rank_diff - 1\n",
    "            else: # Southwest\n",
    "                return DIAGONAL_MOVE_START_IDX + NUM_SINGLE_SQUARE_STEPS + -rank_diff - 1\n",
    "        else:\n",
    "            if rank_diff > 0: # Northeast\n",
    "                return DIAGONAL_MOVE_START_IDX + 2 * NUM_SINGLE_SQUARE_STEPS + rank_diff - 1\n",
    "            else: # Southeast\n",
    "                return DIAGONAL_MOVE_START_IDX + 3 * NUM_SINGLE_SQUARE_STEPS + -rank_diff - 1\n",
    "    \n",
    "    # Knight move\n",
    "    if file_diff < 0:\n",
    "        if rank_diff > 0: # Northwest\n",
    "            if abs_rank_diff > abs_file_diff:\n",
    "                return KNIGHT_MOVE_START_IDX\n",
    "            else:\n",
    "                return KNIGHT_MOVE_START_IDX + 1\n",
    "        else: # Southwest\n",
    "            if abs_rank_diff > abs_file_diff:\n",
    "                return KNIGHT_MOVE_START_IDX + 2\n",
    "            else:\n",
    "                return KNIGHT_MOVE_START_IDX + 3\n",
    "    else:\n",
    "        if rank_diff > 0: # Northeast\n",
    "            if abs_rank_diff > abs_file_diff:\n",
    "                return KNIGHT_MOVE_START_IDX + 4\n",
    "            else:\n",
    "                return KNIGHT_MOVE_START_IDX + 5\n",
    "        else: # Southeast\n",
    "            if abs_rank_diff > abs_file_diff:\n",
    "                return KNIGHT_MOVE_START_IDX + 6\n",
    "            else:\n",
    "                return KNIGHT_MOVE_START_IDX + 7\n",
    "\n",
    "EPS = 1e-3\n",
    "\n",
    "class ChessState:\n",
    "    def __init__(self, board: chess.Board=None, transposition_table: Dict[Hashable, int]=None) -> None:\n",
    "        if board is None:\n",
    "            self.board = chess.Board()\n",
    "        else:\n",
    "            self.board = board\n",
    "        \n",
    "        if transposition_table is None:\n",
    "            self.transposition_table = {get_hash(self.board): 1}\n",
    "        else:\n",
    "            self.transposition_table = transposition_table\n",
    "    \n",
    "    def get_next_state(self, action: chess.Move) -> 'ChessState':\n",
    "        next_board = self.board.copy()\n",
    "        next_board.push(action)\n",
    "\n",
    "        next_transposition_table = self.transposition_table.copy()\n",
    "        next_hash = get_hash(next_board)\n",
    "        if next_hash not in next_transposition_table:\n",
    "            next_transposition_table[next_hash] = 1\n",
    "        else:\n",
    "            next_transposition_table[next_hash] += 1\n",
    "        \n",
    "        return ChessState(next_board, next_transposition_table)\n",
    "    \n",
    "    def get_valid_moves(self) -> chess.LegalMoveGenerator:\n",
    "        return self.board.legal_moves\n",
    "    \n",
    "    def get_value_and_terminated(self) -> Tuple[int, bool]:\n",
    "        \"\"\"Value is from parent's perspective.\"\"\"\n",
    "        outcome = self.board.outcome(claim_draw=True)\n",
    "\n",
    "        if outcome is None:\n",
    "            return 0, False\n",
    "        if outcome.winner is None:\n",
    "            return 0, True\n",
    "        return 1, True\n",
    "    \n",
    "    def get_player(self) -> chess.Color:\n",
    "        return self.board.turn\n",
    "    \n",
    "    def show_state(self, orientation: chess.Color) -> None:\n",
    "        display(SVG(\n",
    "            chess.svg.board(self.board, size=400, orientation=orientation)\n",
    "        ))\n",
    "\n",
    "    def get_encoded_state(self) -> np.ndarray:\n",
    "        if self.get_player() is chess.BLACK:\n",
    "            board = self.board.mirror()\n",
    "        else:\n",
    "            board = self.board\n",
    "        \n",
    "        encoding = []\n",
    "\n",
    "        # Piece positions\n",
    "        for piece in chess.PIECE_TYPES:\n",
    "            encoding.append(np.reshape(board.pieces(piece, chess.WHITE).tolist(), (8, 8)).astype(np.float64))\n",
    "        for piece in chess.PIECE_TYPES:\n",
    "            encoding.append(np.reshape(board.pieces(piece, chess.BLACK).tolist(), (8, 8)).astype(np.float64))\n",
    "\n",
    "        # Castling rights\n",
    "        encoding.append(board.has_kingside_castling_rights(chess.WHITE) * np.ones((8, 8)))\n",
    "        encoding.append(board.has_queenside_castling_rights(chess.WHITE) * np.ones((8, 8)))\n",
    "        encoding.append(board.has_kingside_castling_rights(chess.BLACK) * np.ones((8, 8)))\n",
    "        encoding.append(board.has_queenside_castling_rights(chess.BLACK) * np.ones((8, 8)))\n",
    "\n",
    "        # Number of occurrences (for 3-fold repetition)\n",
    "        encoding.append(self.transposition_table[get_hash(self.board)] * np.ones((8, 8)))\n",
    "        # 50 move rule counter\n",
    "        encoding.append(board.halfmove_clock * np.ones((8, 8)) / 100) # TODO: Do we floor divide halfmove_clock by 2?\n",
    "        # Total move counter\n",
    "        encoding.append(board.fullmove_number * np.ones((8, 8)) / 50)\n",
    "\n",
    "        return np.transpose(np.array(encoding), (1, 2, 0))\n",
    "    \n",
    "    def decode_policy(self, policy: np.ndarray) -> Dict[chess.Move, float]:\n",
    "        \"\"\"Convert policy (given as if we were white) of shape (8, 8, 73) to dictionary mapping legal moves to probabilities.\"\"\"\n",
    "        is_black = self.get_player() is chess.BLACK\n",
    "        probs: Dict[chess.Move, float] = {}\n",
    "        total_prob = 0\n",
    "\n",
    "        for move in self.get_valid_moves():\n",
    "            from_rank, from_file = chess.square_rank(move.from_square), chess.square_file(move.from_square)\n",
    "            if self.get_player() is chess.BLACK:\n",
    "                from_rank = 7 - from_rank\n",
    "            prob = policy[from_rank, from_file, move_to_policy_idx(move, is_black)]\n",
    "            probs[move] = prob\n",
    "            total_prob += prob\n",
    "\n",
    "        for move, prob in probs.items():\n",
    "            probs[move] = prob / (total_prob + EPS)\n",
    "\n",
    "        return probs\n",
    "    \n",
    "    def encode_policy(self, probs: Dict[chess.Move, float]) -> np.ndarray:\n",
    "        \"\"\"Convert dictionary mapping legal move to probabilities to policy of shape (8, 8, 73) as if we were white.\"\"\"\n",
    "        is_black = self.get_player() is chess.BLACK\n",
    "        policy = np.zeros((8, 8, 73))\n",
    "\n",
    "        for move, prob in probs.items():\n",
    "            from_rank, from_file = chess.square_rank(move.from_square), chess.square_file(move.from_square)\n",
    "            if is_black:\n",
    "                from_rank = 7 - from_rank\n",
    "\n",
    "            policy[from_rank, from_file, move_to_policy_idx(move, is_black)] = prob\n",
    "\n",
    "        return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, state: ChessState, args: dict, parent: 'Node'=None, action_taken: chess.Move=None, prior: float=0, visit_count: int=0) -> None:\n",
    "        self.state = state\n",
    "        self.args = args\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.prior = prior\n",
    "\n",
    "        self.children: List['Node'] = []\n",
    "\n",
    "        self.visit_count = visit_count\n",
    "        self.value_sum = 0\n",
    "\n",
    "    def is_fully_expanded(self) -> bool:\n",
    "        return len(self.children) > 0\n",
    "    \n",
    "    def select(self) -> 'Node':\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "\n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_ucb = ucb\n",
    "                best_child = child\n",
    "\n",
    "        return best_child\n",
    "\n",
    "    def get_ucb(self, child: 'Node') -> float:\n",
    "        if child.visit_count == 0:\n",
    "            q = 0\n",
    "        else:\n",
    "            # value_sum / visit_count can be between -1 and 1, we want between 0 and 1 like probability\n",
    "            # child's perspective is opposite self's perspective so we flip with 1 - q\n",
    "            q = 1 - (child.value_sum / child.visit_count + 1) / 2\n",
    "        return q + self.args['C'] * np.sqrt(self.visit_count) / (1 + child.visit_count) * child.prior\n",
    "    \n",
    "    def expand(self, policy: Dict[chess.Move, float]):\n",
    "        for action, prob in policy.items():\n",
    "            if prob > 0:\n",
    "                child_state = self.state.board.copy()\n",
    "                child_state = self.state.get_next_state(action)\n",
    "                \n",
    "                child = Node(child_state, self.args, self, action, prob)\n",
    "                self.children.append(child)\n",
    "            \n",
    "    def backprop(self, value: int) -> None:\n",
    "        self.visit_count += 1\n",
    "        self.value_sum += value\n",
    "\n",
    "        if self.parent is not None:\n",
    "            self.parent.backprop(value * -1)\n",
    "\n",
    "class Model(ABC):\n",
    "    @abstractmethod\n",
    "    def __call__(self, state: ChessState) -> Tuple[Dict[chess.Move, float], float]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(\n",
    "            self,\n",
    "            states: List[np.ndarray],\n",
    "            policies: List[np.ndarray], \n",
    "            values: List[np.ndarray],\n",
    "            epochs: int,\n",
    "            callbacks: List[tf.keras.callbacks.Callback]=[]\n",
    "        ) -> Any:\n",
    "        pass\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, args: dict, model: Model) -> None:\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "\n",
    "    def search(self, state: ChessState) -> Dict[chess.Move, float]:\n",
    "        root = Node(state, self.args, visit_count=1)\n",
    "\n",
    "        policy, _ = self.model(state)\n",
    "        eps, alpha = self.args['dirichlet_eps'], self.args['dirichlet_alpha']\n",
    "        rng = np.random.default_rng()\n",
    "        total_prob = 0\n",
    "        for move in policy:\n",
    "            policy[move] = (1 - eps) * policy[move] + eps * rng.dirichlet([alpha], 1)\n",
    "            total_prob += policy[move]\n",
    "        for move in policy:\n",
    "            policy[move] /= total_prob\n",
    "\n",
    "        root.expand(policy)\n",
    "\n",
    "        for _ in range(self.args['num_searches']):\n",
    "            # Selection\n",
    "            node = root\n",
    "            while node.is_fully_expanded():\n",
    "                node = node.select()\n",
    "            \n",
    "            value, is_terminal = node.state.get_value_and_terminated()\n",
    "            # value is returned from node.parent's perspective\n",
    "            # if node.parent's perspective is same as root's perspective, then flip sign so that node's value is correct\n",
    "            # i.e., if node's perspective is different from root's presepctive, then flip sign\n",
    "            if node.state.get_player() is not root.state.get_player():\n",
    "                value *= -1\n",
    "\n",
    "            if not is_terminal:\n",
    "                # policy and value are from node's perspective\n",
    "                policy, value = self.model(node.state)\n",
    "                node.expand(policy)\n",
    "\n",
    "            # Backprop\n",
    "            node.backprop(value)\n",
    "\n",
    "        # Return normalized visit counts\n",
    "        visit_count = {move: 0 for move in root.state.get_valid_moves()}\n",
    "        total_visit_count = 0\n",
    "        for child in root.children:\n",
    "            visit_count[child.action_taken] = child.visit_count\n",
    "            total_visit_count += child.visit_count\n",
    "\n",
    "        action_probs = {move: visit_count[move] / total_visit_count for move in visit_count}\n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stockfish(Model):\n",
    "    def __init__(self) -> None:\n",
    "        self.model = chess.engine.SimpleEngine.popen_uci('/opt/homebrew/opt/stockfish/bin/stockfish')\n",
    "\n",
    "    def __call__(self, state: ChessState) -> Tuple[Dict[chess.Move, float], float]:\n",
    "        \"\"\"Policy and value are given from this node's perspective.\"\"\"\n",
    "        value = self.model.analyse(state.board, chess.engine.Limit(depth=0))['score'].white().wdl().expectation() * 2 - 1\n",
    "        if state.get_player() is not chess.WHITE:\n",
    "            value *= -1\n",
    "\n",
    "        next_values = {}\n",
    "        total_value = 0\n",
    "\n",
    "        for move in state.get_valid_moves():\n",
    "            next_board = state.board.copy()\n",
    "            next_board.push(move)\n",
    "            next_value = self.model.analyse(next_board, chess.engine.Limit(depth=0))['score'].white().wdl().expectation()\n",
    "            if state.get_player() is not chess.WHITE:\n",
    "                next_value = 1 - next_value\n",
    "            \n",
    "            next_values[move] = next_value\n",
    "            total_value += next_value\n",
    "\n",
    "        if total_value == 0:\n",
    "            return {}, value\n",
    "\n",
    "        policy = {move: next_values[move] / total_value for move in next_values}\n",
    "\n",
    "        return policy, value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCTS Test with Stockfish Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = ChessState()\n",
    "args = {\n",
    "    # 'C': np.sqrt(2),\n",
    "    'C': 2,\n",
    "    'num_searches': 1000,\n",
    "    'dirichlet_eps': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "model = Stockfish()\n",
    "mcts = MCTS(args, model)\n",
    "human = chess.BLACK\n",
    "\n",
    "while True:\n",
    "    clear_output()\n",
    "    state.show_state(human)\n",
    "\n",
    "    if state.get_player() is human:\n",
    "        while True:\n",
    "            try:\n",
    "                action = str(input('Move (algebraic notation): '))\n",
    "                if action == 'quit':\n",
    "                    break\n",
    "                action = state.board.parse_san(action)\n",
    "            except:\n",
    "                continue\n",
    "            break\n",
    "    else:\n",
    "        mcts_probs = mcts.search(state)\n",
    "        action = max(mcts_probs, key=mcts_probs.get)\n",
    "\n",
    "    state = state.get_next_state(action)\n",
    "\n",
    "    value, is_terminal = state.get_value_and_terminated()\n",
    "    if is_terminal:\n",
    "        clear_output()\n",
    "        state.show_state(human)\n",
    "        if value == 0:\n",
    "            print('Draw')\n",
    "        else:\n",
    "            # Winner is the previous player\n",
    "            winner = 'White' if state.get_player() is not chess.WHITE else 'Black'\n",
    "            print(f'{winner} won')\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_block = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(num_filters, (3, 3), padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(axis=-1),\n",
    "            tf.keras.layers.Activation('relu'),\n",
    "            tf.keras.layers.Conv2D(num_filters, kernel_size=(3, 3), padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(axis=-1)\n",
    "        ])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.relu = tf.keras.layers.Activation('relu')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs_skip = inputs\n",
    "\n",
    "        inputs = self.conv_block(inputs)\n",
    "        inputs = self.add([inputs, inputs_skip])\n",
    "        inputs = self.relu(inputs)\n",
    "\n",
    "        return inputs\n",
    "\n",
    "class ResNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.torso = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(256, (3, 3), padding='same', activation='relu', input_shape=(8, 8, 19)),\n",
    "            ResNetBlock(256)\n",
    "        ])\n",
    "        self.policy_head = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(256, (1, 1), activation='relu'),\n",
    "            tf.keras.layers.Conv2D(73, (1, 1), activation=None),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Activation('softmax')\n",
    "        ])\n",
    "        self.value_head = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(1, (1, 1), activation='relu'),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.Dense(1, activation='tanh')\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        torso_output = self.torso(inputs)\n",
    "        policy = self.policy_head(torso_output)\n",
    "        value = self.value_head(torso_output)\n",
    "        return {'policy_output': policy, 'value_output': value}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-04 18:16:34.910207: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2023-07-04 18:16:34.910231: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2023-07-04 18:16:34.910238: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2023-07-04 18:16:34.910275: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-07-04 18:16:34.910295: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"res_net\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_1 (Sequential)   (None, 8, 8, 256)         1226240   \n",
      "                                                                 \n",
      " sequential_2 (Sequential)   (None, 4672)              84553     \n",
      "                                                                 \n",
      " sequential_3 (Sequential)   (None, 1)                 17154     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1327947 (5.07 MB)\n",
      "Trainable params: 1326923 (5.06 MB)\n",
      "Non-trainable params: 1024 (4.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ResNet()\n",
    "model.build((None, 8, 8, 19))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetModel(Model):\n",
    "    def __init__(self) -> None:\n",
    "        self.model = ResNet()\n",
    "        self.model.compile(\n",
    "            optimizer='Adam',\n",
    "            loss={'policy_output': 'categorical_crossentropy', 'value_output': 'mean_squared_error'},\n",
    "            metrics={'policy_output': 'categorical_accuracy', 'value_output': 'mean_squared_error'}\n",
    "        )\n",
    "\n",
    "    def __call__(self, state: ChessState) -> Tuple[Dict[chess.Move, float], float]:\n",
    "        \"\"\"Policy and value are given from this node's perspective.\"\"\"\n",
    "        outputs = self.model(np.array([state.get_encoded_state()]))\n",
    "        policy, value = outputs['policy_output'], outputs['value_output']\n",
    "        policy = np.reshape(policy, (8, 8, -1))\n",
    "        return state.decode_policy(policy), value.numpy().item()\n",
    "    \n",
    "    def fit(\n",
    "            self,\n",
    "            states: List[np.ndarray],\n",
    "            policies: List[np.ndarray], \n",
    "            values: List[np.ndarray],\n",
    "            epochs: int,\n",
    "            callbacks: List[tf.keras.callbacks.Callback]=[]\n",
    "        ) -> Any:\n",
    "        \n",
    "        return self.model.fit(\n",
    "            x=np.array(states),\n",
    "            y={'policy_output': np.array(policies).reshape((len(policies), -1)), 'value_output': np.array(values)},\n",
    "            epochs=epochs,\n",
    "            validation_split=0,\n",
    "            shuffle=True,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Engine:\n",
    "    def __init__(self, model: Model, args) -> None:\n",
    "        self.model = model\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(args, model)\n",
    "\n",
    "    def self_play(self) -> Tuple[List[np.ndarray], List[np.ndarray], List[int]]:\n",
    "        state_history: List[ChessState] = []\n",
    "        policy_history: List[np.ndarray] = []\n",
    "\n",
    "        state = ChessState()\n",
    "        while True:\n",
    "            action_probs = self.mcts.search(state)\n",
    "\n",
    "            state_history.append(state)\n",
    "            policy_history.append(state.encode_policy(action_probs).flatten())\n",
    "\n",
    "            # Higher temperature => squishes probabilities together => encourages more exploration\n",
    "            temperature_action_probs = np.array(action_probs.values()) ** (1 / self.args['temperature'])\n",
    "            action = np.random.choice(list(action_probs.keys()), p=temperature_action_probs)\n",
    "            state = state.get_next_state(action)\n",
    "            value, is_terminal = state.get_value_and_terminated()\n",
    "\n",
    "            if is_terminal:\n",
    "                return (\n",
    "                    [past_state.get_encoded_state() for past_state in state_history],\n",
    "                    policy_history,\n",
    "                    [value if past_state.get_player() is state.get_player() else -value for past_state in state_history]\n",
    "                )\n",
    "\n",
    "    def learn(self):\n",
    "        for i in range(self.args['num_learn_iters']):\n",
    "            state_memory: List[np.ndarray] = []\n",
    "            policy_memory: List[np.ndarray] = []\n",
    "            value_memory: List[int] = []\n",
    "\n",
    "            # Self-play\n",
    "            for _ in trange(self.args['num_self_play_iters'], desc='Self-play'):\n",
    "                state_history: List[ChessState] = []\n",
    "                policy_history: List[np.ndarray] = []\n",
    "\n",
    "                state = ChessState()\n",
    "                while True:\n",
    "                    action_probs = self.mcts.search(state)\n",
    "\n",
    "                    state_history.append(state)\n",
    "                    policy_history.append(state.encode_policy(action_probs))\n",
    "\n",
    "                    action = np.random.choice(list(action_probs.keys()), p=list(action_probs.values()))\n",
    "                    state = state.get_next_state(action)\n",
    "                    value, is_terminal = state.get_value_and_terminated()\n",
    "\n",
    "                    if is_terminal:\n",
    "                        encoded_state_history = [past_state.get_encoded_state() for past_state in state_history]\n",
    "                        value_history = [value if past_state.get_player() is state.get_player() else -value for past_state in state_history]\n",
    "                        break\n",
    "\n",
    "                state_memory += encoded_state_history\n",
    "                policy_memory += policy_history\n",
    "                value_memory += value_history\n",
    "\n",
    "            # Train\n",
    "            cp_callback = tf.keras.callbacks.ModelCheckpoint(f'learning/cp{i}.ckpt')\n",
    "            self.model.fit(state_memory, policy_memory, value_memory, self.args['num_epochs'], [cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self-play: 100%|██████████| 3/3 [00:13<00:00,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2023-07-04 18:19:00.033846: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - ETA: 0s - loss: 4.4757 - policy_output_loss: 3.5417 - value_output_loss: 0.9339 - policy_output_categorical_accuracy: 0.3221 - value_output_mean_squared_error: 0.9339 INFO:tensorflow:Assets written to: learning/cp0.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: learning/cp0.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 2s 159ms/step - loss: 4.4757 - policy_output_loss: 3.5417 - value_output_loss: 0.9339 - policy_output_categorical_accuracy: 0.3221 - value_output_mean_squared_error: 0.9339\n",
      "Epoch 2/3\n",
      "7/9 [======================>.......] - ETA: 0s - loss: 1.8912 - policy_output_loss: 1.0870 - value_output_loss: 0.8042 - policy_output_categorical_accuracy: 0.6830 - value_output_mean_squared_error: 0.8042INFO:tensorflow:Assets written to: learning/cp0.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: learning/cp0.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 1s 136ms/step - loss: 1.7791 - policy_output_loss: 1.0078 - value_output_loss: 0.7713 - policy_output_categorical_accuracy: 0.7154 - value_output_mean_squared_error: 0.7713\n",
      "Epoch 3/3\n",
      "7/9 [======================>.......] - ETA: 0s - loss: 1.0681 - policy_output_loss: 0.5805 - value_output_loss: 0.4876 - policy_output_categorical_accuracy: 0.8125 - value_output_mean_squared_error: 0.4876INFO:tensorflow:Assets written to: learning/cp0.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: learning/cp0.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 1s 126ms/step - loss: 1.0006 - policy_output_loss: 0.5371 - value_output_loss: 0.4635 - policy_output_categorical_accuracy: 0.8277 - value_output_mean_squared_error: 0.4635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self-play: 100%|██████████| 3/3 [01:04<00:00, 21.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " 3/24 [==>...........................] - ETA: 0s - loss: 2.9893 - policy_output_loss: 2.8304 - value_output_loss: 0.1590 - policy_output_categorical_accuracy: 0.3542 - value_output_mean_squared_error: 0.1590"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - ETA: 0s - loss: 2.2505 - policy_output_loss: 2.1889 - value_output_loss: 0.0616 - policy_output_categorical_accuracy: 0.5413 - value_output_mean_squared_error: 0.0616INFO:tensorflow:Assets written to: learning/cp1.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: learning/cp1.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 2s 65ms/step - loss: 2.2505 - policy_output_loss: 2.1889 - value_output_loss: 0.0616 - policy_output_categorical_accuracy: 0.5413 - value_output_mean_squared_error: 0.0616\n",
      "Epoch 2/3\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 1.3053 - policy_output_loss: 1.2933 - value_output_loss: 0.0120 - policy_output_categorical_accuracy: 0.7273 - value_output_mean_squared_error: 0.0120INFO:tensorflow:Assets written to: learning/cp1.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: learning/cp1.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 60ms/step - loss: 1.2538 - policy_output_loss: 1.2418 - value_output_loss: 0.0120 - policy_output_categorical_accuracy: 0.7375 - value_output_mean_squared_error: 0.0120\n",
      "Epoch 3/3\n",
      "22/24 [==========================>...] - ETA: 0s - loss: 0.7333 - policy_output_loss: 0.7217 - value_output_loss: 0.0115 - policy_output_categorical_accuracy: 0.8153 - value_output_mean_squared_error: 0.0115INFO:tensorflow:Assets written to: learning/cp1.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: learning/cp1.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 55ms/step - loss: 0.7301 - policy_output_loss: 0.7187 - value_output_loss: 0.0114 - policy_output_categorical_accuracy: 0.8173 - value_output_mean_squared_error: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Self-play: 100%|██████████| 3/3 [00:07<00:00,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.5122 - policy_output_loss: 1.4811 - value_output_loss: 0.0310 - policy_output_categorical_accuracy: 0.5312 - value_output_mean_squared_error: 0.0310"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - ETA: 0s - loss: 1.1537 - policy_output_loss: 1.1323 - value_output_loss: 0.0215 - policy_output_categorical_accuracy: 0.6780 - value_output_mean_squared_error: 0.0215INFO:tensorflow:Assets written to: learning/cp2.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: learning/cp2.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 245ms/step - loss: 1.1537 - policy_output_loss: 1.1323 - value_output_loss: 0.0215 - policy_output_categorical_accuracy: 0.6780 - value_output_mean_squared_error: 0.0215\n",
      "Epoch 2/3\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 0.4629 - policy_output_loss: 0.4545 - value_output_loss: 0.0084 - policy_output_categorical_accuracy: 0.8516 - value_output_mean_squared_error: 0.0084INFO:tensorflow:Assets written to: learning/cp2.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: learning/cp2.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 192ms/step - loss: 0.3698 - policy_output_loss: 0.3630 - value_output_loss: 0.0068 - policy_output_categorical_accuracy: 0.8870 - value_output_mean_squared_error: 0.0068\n",
      "Epoch 3/3\n",
      "4/6 [===================>..........] - ETA: 0s - loss: 0.1918 - policy_output_loss: 0.1865 - value_output_loss: 0.0053 - policy_output_categorical_accuracy: 0.9688 - value_output_mean_squared_error: 0.0053INFO:tensorflow:Assets written to: learning/cp2.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: learning/cp2.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 176ms/step - loss: 0.1784 - policy_output_loss: 0.1734 - value_output_loss: 0.0050 - policy_output_categorical_accuracy: 0.9661 - value_output_mean_squared_error: 0.0050\n"
     ]
    }
   ],
   "source": [
    "model = ResNetModel()\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 3,\n",
    "    'num_learn_iters': 3,\n",
    "    'num_self_play_iters': 3,\n",
    "    'num_epochs': 3,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_eps': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "engine = Engine(model, args)\n",
    "engine.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
